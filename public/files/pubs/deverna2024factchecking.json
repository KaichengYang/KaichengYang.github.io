{
  "authors": [
    {"id": "mdeverna"},
    {"id": "hyan"},
    {"id": "kcyang", "annotation": ["highlight"]},
    {"id": "fmenczer"}
  ],
  "year": [
    2023
  ],
  "date": "2024-08-07",
  "type": [
    "preprint"
  ],
  "topic": [
    "misinformation",
    "genai"
  ],
  "highlight": [
    "highlight"
  ],
  "links": [
    {
      "url": "https://arxiv.org/abs/2308.10800",
      "name": "arXiv"
    },
    {
      "url": "https://github.com/osome-iu/AI_fact_checking",
      "name": "GitHub"
    },
    {
      "url": "https://x.com/mdeverna2/status/1693947115787969013",
      "name": "Twitter"
    }
  ],
  "altmetric": {
    "arxiv_id": "2308.10800"
  },
  "bibtex_string": "@misc{deverna2024factchecking,\n\ttitle={Fact-checking information from large language models can decrease headline discernment},\n\tauthor={Matthew R. DeVerna and Harry Yaojun Yan and Kai-Cheng Yang and Filippo Menczer},\n\tyear={2024},\n\teprint={2308.10800},\n\tarchivePrefix={arXiv},\n\tprimaryClass={cs.HC},\n\turl={https://arxiv.org/abs/2308.10800},\n\tjournal={arXiv:2308.10800}}",
  "abstract": "Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here, we investigate the impact of fact-checking information generated by a popular large language model (LLM) on belief in, and sharing intent of, political news headlines in a preregistered randomized control experiment. Although the LLM accurately identifies most false headlines (90%), we find that this information does not significantly improve participants' ability to discern headline accuracy or share accurate news. In contrast, viewing human-generated fact checks enhances discernment in both cases. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs in false headlines that it is unsure about. On the positive side, AI fact-checking information increases the sharing intent for correctly labeled true headlines. When participants are given the option to view LLM fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false headlines. Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences."
}